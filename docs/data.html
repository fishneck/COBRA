<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Dataset</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo">Quantifying Impairment and Disease Severity Using AI Models Trained on Healthy Subjects</a>
									<ul class="icons">
										<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="https://github.com/fishneck/COBRA" class="icon brands fa-github"><span class="label">Github</span></a></li>
									</ul>
								</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1>Dataset</h1>
									</header>

									<p>Automatic assessment of impairment and disease severity is a key challenge in data-driven medicine. We propose a novel framework to address this challenge, which leverages AI models trained exclusively on healthy individuals. The COnfidence-Based chaRacterization of Anomalies (COBRA) score exploits the decrease in confidence of these models when presented with impaired or diseased patients to quantify their deviation from the healthy population.</p>
									<p>We applied the COBRA score to address a key limitation of current clinical evaluation of upper-body impairment in stroke patients, as well as to quantify severity of knee osteoarthritis from magnetic-resonance imaging scans.</p>
									<p>Raw data and our AI model outputs for two slinical applications are shared for research use. For the quantification of stroke-related motor impairment, we leverage the <strong>StrokeRehab</strong> dataset. For the quantification of knee osteoarthritis severity, we leverage the <strong>OAI-ZIB</strong> dataset.</p>

									<hr class="major" />

									<h2>StrokeRehab</h2>
									<p>StrokeRehab is a large-scale, multimodal dataset that serves as a new benchmark for recognizing elemental short-duration actions at high temporal resolution. Designed to address the challenges of automatic action identification from video and kinematic data, StrokeRehab is particularly tailored for applications requiring detailed motion analysis, such as arm rehabilitation post-stroke.</p>
									<p>It consists of high-quality inertial measurement unit sensor and video data of 51 stroke-impaired patients and 29 healthy subjects performing activities of daily living like feeding, brushing teeth, etc. Because it contains data from both healthy and impaired individuals, StrokeRehab can be used to study the influence of distribution shift in action-recognition tasks.</p>

									<span class="image main"><img src="images/Data-StrokeRehab.png" alt="" /></span>

									<p> <strong>IMU Data </strong>: Nine IMUs are attached to the upper body, specifically the cervical vertebra C7, the thoracic vertebra T12, the pelvis, and both arms, forearms, and hands which capture 3D linear accelerations and angular velocities at 100 Hz. These IMUs captured 76-dimensional kinematic features of 3D linear accelerations, 3D quaternions, and joint angles from the upper body. Angular velocities are converted to sensor-centric unit quaternions, representing the rotation of each sensor on its own axes, with coordinate transformation matrices</p>

									<p> <strong>Video Data </strong>: Video data were synchronously captured using two high definition cameras (1088 x 704, 60 frames per second or 100 frames per second; Ninox, Noraxon) placed orthogonally < 2 m from the subject. The video below shows an example capture of dataset.</p>

									<p> <strong>Data Access</strong>: We have released the kinectic (IMU) dataset and video features on the <a href="https://simtk.org/plugins/datashare/index.php?group_id=2269&amp;login=1#">simTK platform</a>. One can freely create an account and access the dataset. We also released model output for the healthy-trained AI models via Google Drive (<a href="https://drive.google.com/drive/folders/1YBgIZJhYRgd7IiChn7yWOsT6HCIKYPhl?usp=drive_link">IMU output</a>, <a href="https://drive.google.com/drive/folders/1tbpq0z6C5aGIdJRrIuF_jAAoN8SWc3KZ?usp=share_link">Video output</a>).</p>

									<hr class="major" />

									<h2>OAI-ZIB</h2>

									
									<p>OAI-ZIB dataset is a collection of MRI (Magnetic Resonance Imaging) data obtained from the Osteoarthritis Initiative (OAI) study, along with manual segmentations of knee structures performed by experts. The MRI scans in the OAI-ZIB dataset offer detailed information about knee anatomy and pathology. These high-quality images enable precise analysis of bone, cartilage, and other soft tissue structures affected by OA.</p>
									<span class="image main"><img src="images/Data-KneeOA.png" alt="" /></span>

									<p>It provides 3D MRI scans of 101 healthy right knees and 378 right knees affected by knee osteoarthritis, a long-term degenerative joint condition. Each knee is labeled with the corresponding Kellgren-Lawrence (KL) grade. The KL grade quantifies OA severity on a scale from 0 (healthy) to 4 (severe), as illustrated in the figure. Each voxel in the MRI scans is labeled to indicate the corresponding tissue (tibia bone, tibia cartilage, femur bone, femur cartilage or background).</p>

									<p> <strong>Data Access</strong>: The MRI raw data and maual segmentations can be downloaded via <a href="https://pubdata.zib.de/">ZIB platform</a>. One can freely create an account and fowwlow instructions to apply for dataset access. We also released model weight for the healthy-trained AI models via Google Drive (<a href="https://drive.google.com/drive/folders/1cBWEblKSqg1uN88ZRWC7ikKmOLTYa-HC?usp=drive_link">model weight</a>). In addition, we provided 5 <a href="https://drive.google.com/drive/folders/1KK473GI1OF2U44euHYA9fVIxsYKoTZsW?usp=drive_link">sample outputs</a>. </p>
								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">Main</a></li>
										<li>
											<span class="opener">Code</span>
											<ul>
												<li><a href="https://github.com/fishneck/COBRA">Repository</a></li>
												<li><a href="https://github.com/fishneck/COBRA/blob/main/models/stroke_video/2%20-%20Generate_plots.ipynb">Demo - Quantification of impairment in stroke patients from video data</a></li>
											</ul>
										</li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Video</h2>
									</header>
									<div class="mini-posts">
										<article>
											<a href="https://youtu.be/5KHjrA8jhTQ?si=SWFimkgIcnclyteR" class="image"><img src="images/Video.png" alt="" /></a>
											<p>The video is available <a href="https://youtu.be/5KHjrA8jhTQ?si=SWFimkgIcnclyteR">here</a>.</p>
										</article>
									</div>
									<ul class="actions">
										<li><a href="https://youtu.be/5KHjrA8jhTQ?si=SWFimkgIcnclyteR" class="button">Watch</a></li>
									</ul>
								</section>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>References</h2>
									</header>
									<p>Yu, B., Kaku, A., Liu, K., Parnandi, A., Fokas, E., Venkatesan, A., Pandit, N., Ranganath, R., Schambra, H. and Fernandez-Granda, C., <a href="https://www.nature.com/articles/s41746-024-01173-x" >Quantifying impairment and disease severity using AI models trained on healthy subjects.</a> npj Digit. Med. 7, 180 (2024).</p>
									<p>Parnandi, Avinash, et al. <strong>"Data-Driven Quantitation of Movement Abnormality after Stroke.”</strong> <em>Bioengineering</em> (2023) </p>
									<p>Kaku, Aakash, et al. <strong>"StrokeRehab: A Benchmark Dataset for Sub-second Action Identification."</strong> <em> Advances in Neural Information Processing Systems(NeurIPS</em> 2022) </p>
									<p>Parnandi, Avinash, et al. <strong>"PrimSeq: A deep learning-based pipeline to quantitate rehabilitation training.”</strong> <em> PLOS Digital Health</em> (2022) </p>
									<p>Kaku, Aakash, et al. <strong>"Towards data-driven stroke rehabilitation via wearable sensors and deep learning."</strong> <em> Machine Learning for Healthcare Conference.</em> PMLR, 2020 </p>
								</section>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Acknowledgements</h2>
									</header>
									<p> This work was supported by NIH grant R01 LM013316, Alzheimer’s Association grant AARG-NTF-21-848627, NSF grant NRT-1922658, and NSF CAREER Award 2145542. </p>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; All rights reserved. Demo Images: <a href="https://unsplash.com">Unsplash</a>. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>